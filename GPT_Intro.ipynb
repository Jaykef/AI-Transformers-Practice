{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to GPTs\n",
    "GPT (Generative Pre-trained Transformer) is a family of state-of-the-art language models developed by OpenAI. They are pre-trained on massive amounts of text data using a self-supervised learning approach, allowing them to generate coherent and high-quality natural language text.\n",
    "\n",
    "Here's some sample code for using the GPT-2 model in Python with the Hugging Face Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Generate text\n",
    "input_text = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "\n",
    "# Convert output to text\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we load the GPT-2 tokenizer and model using the Hugging Face Transformers library. We then generate text by feeding in an initial input sequence and using the generate() method. Finally, we convert the output tensor to text using the tokenizer's decode() method.\n",
    "\n",
    "# The Transformer Paper (Attension is all You Need)\n",
    "The groundbreaking transformer paper \"Attention Is All You Need\" introduced a new neural network architecture for sequence-to-sequence tasks such as machine translation, text summarization, and language modeling. Unlike previous architectures that relied on recurrent neural networks (RNNs) or convolutional neural networks (CNNs), the transformer architecture used only self-attention mechanisms to process input sequences. This allowed for much faster training and inference times, while also achieving state-of-the-art performance on several benchmarks.\n",
    "\n",
    "Simple implementation of The Transformer Architecture in Python(PyTorch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size, num_layers):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, hidden_size)\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, hidden_size)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(hidden_size, num_heads=8) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(hidden_size, num_heads=8) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        src = self.encoder_embedding(src)\n",
    "        tgt = self.decoder_embedding(tgt)\n",
    "        \n",
    "        src_mask = self._generate_square_subsequent_mask(src.shape[1])\n",
    "        tgt_mask = self._generate_square_subsequent_mask(tgt.shape[1])\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        for layer in self.decoder_layers:\n",
    "            tgt = layer(tgt, src, tgt_mask, src_mask)\n",
    "        \n",
    "        output = self.fc(tgt)\n",
    "        return output\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation defines a Transformer class that takes in an input vocabulary size, output vocabulary size, hidden size, and number of layers as arguments. The forward method performs the main computation of the transformer, using self-attention mechanisms in both the encoder and decoder layers. The _generate_square_subsequent_mask method generates a mask to prevent the decoder from attending to future tokens in the output sequence.\n",
    "\n",
    "The input sequence is first embedded into a high-dimensional vector space.\n",
    "The embedded input is then fed into a series of encoder layers. Each encoder layer contains a self-attention mechanism that allows the model to focus on different parts of the input sequence at each layer.\n",
    "\n",
    "The output of the final encoder layer is passed through a decoder, which also consists of multiple layers with self-attention mechanisms. The decoder generates the output sequence one token at a time, conditioned on the input sequence and all previously generated tokens.\n",
    "\n",
    "While this implementation is simplified compared to the full transformer architecture described in the paper, it demonstrates the core ideas of using self-attention mechanisms for sequence-to-sequence tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
